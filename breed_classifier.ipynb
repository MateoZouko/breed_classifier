{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsrobDWmKqLE",
        "outputId": "98dc89ae-9825-4228-e93e-e813e7f28960"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hcZUKXygJJX1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = '/content/drive/MyDrive/dogs'\n",
        "\n",
        "data_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = data_gen.flow_from_directory(\n",
        "    data_set,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = data_gen.flow_from_directory(\n",
        "    data_set,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yV7KVScCJTPn",
        "outputId": "2d952bff-6fed-422c-dba0-13acde131277"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 64 images belonging to 4 classes.\n",
            "Found 16 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    conv_base,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size if train_generator.samples // train_generator.batch_size > 0 else 1,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size if validation_generator.samples // validation_generator.batch_size > 0 else 1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y0Ky5rPzJ27W",
        "outputId": "b64db7b3-b989-4f01-83ba-bf039087ae5f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "3/3 [==============================] - 6s 1s/step - loss: 4.7852 - accuracy: 0.2045 - val_loss: 2.6138 - val_accuracy: 0.3125\n",
            "Epoch 2/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 3.3436 - accuracy: 0.2833 - val_loss: 3.1071 - val_accuracy: 0.2500\n",
            "Epoch 3/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 2.8505 - accuracy: 0.2500 - val_loss: 1.3122 - val_accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "3/3 [==============================] - 4s 2s/step - loss: 1.9263 - accuracy: 0.3864 - val_loss: 1.6815 - val_accuracy: 0.3750\n",
            "Epoch 5/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.6257 - accuracy: 0.4545 - val_loss: 1.5912 - val_accuracy: 0.2500\n",
            "Epoch 6/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.8664 - accuracy: 0.4318 - val_loss: 1.0403 - val_accuracy: 0.6875\n",
            "Epoch 7/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.2232 - accuracy: 0.5227 - val_loss: 2.1219 - val_accuracy: 0.2500\n",
            "Epoch 8/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.5333 - accuracy: 0.4773 - val_loss: 1.1040 - val_accuracy: 0.3750\n",
            "Epoch 9/30\n",
            "3/3 [==============================] - 4s 2s/step - loss: 1.2059 - accuracy: 0.5000 - val_loss: 1.0585 - val_accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.3116 - accuracy: 0.5833 - val_loss: 1.1622 - val_accuracy: 0.6250\n",
            "Epoch 11/30\n",
            "3/3 [==============================] - 3s 976ms/step - loss: 1.0811 - accuracy: 0.6364 - val_loss: 1.1010 - val_accuracy: 0.5625\n",
            "Epoch 12/30\n",
            "3/3 [==============================] - 5s 2s/step - loss: 1.0667 - accuracy: 0.6167 - val_loss: 0.9846 - val_accuracy: 0.5625\n",
            "Epoch 13/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.9666 - accuracy: 0.5000 - val_loss: 0.9307 - val_accuracy: 0.5625\n",
            "Epoch 14/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.0797 - accuracy: 0.5227 - val_loss: 0.9844 - val_accuracy: 0.5625\n",
            "Epoch 15/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.7961 - accuracy: 0.7045 - val_loss: 0.9695 - val_accuracy: 0.5625\n",
            "Epoch 16/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.0384 - accuracy: 0.6167 - val_loss: 0.8265 - val_accuracy: 0.6875\n",
            "Epoch 17/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.9935 - accuracy: 0.6136 - val_loss: 0.8432 - val_accuracy: 0.6250\n",
            "Epoch 18/30\n",
            "3/3 [==============================] - 5s 2s/step - loss: 0.5945 - accuracy: 0.7727 - val_loss: 0.7109 - val_accuracy: 0.6875\n",
            "Epoch 19/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.6196 - accuracy: 0.7500 - val_loss: 0.8249 - val_accuracy: 0.7500\n",
            "Epoch 20/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.6921 - accuracy: 0.7333 - val_loss: 1.0073 - val_accuracy: 0.5000\n",
            "Epoch 21/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.4713 - accuracy: 0.8636 - val_loss: 0.9036 - val_accuracy: 0.6250\n",
            "Epoch 22/30\n",
            "3/3 [==============================] - 5s 2s/step - loss: 0.6233 - accuracy: 0.7833 - val_loss: 0.7312 - val_accuracy: 0.7500\n",
            "Epoch 23/30\n",
            "3/3 [==============================] - 4s 2s/step - loss: 0.6675 - accuracy: 0.7500 - val_loss: 0.7425 - val_accuracy: 0.7500\n",
            "Epoch 24/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.5841 - accuracy: 0.7273 - val_loss: 0.7333 - val_accuracy: 0.6250\n",
            "Epoch 25/30\n",
            "3/3 [==============================] - 4s 2s/step - loss: 0.7190 - accuracy: 0.7727 - val_loss: 0.6107 - val_accuracy: 0.6875\n",
            "Epoch 26/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.4278 - accuracy: 0.8636 - val_loss: 0.7966 - val_accuracy: 0.6875\n",
            "Epoch 27/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.4161 - accuracy: 0.8333 - val_loss: 0.8332 - val_accuracy: 0.5625\n",
            "Epoch 28/30\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.5629 - accuracy: 0.7727 - val_loss: 0.8285 - val_accuracy: 0.6875\n",
            "Epoch 29/30\n",
            "3/3 [==============================] - 4s 2s/step - loss: 0.4398 - accuracy: 0.8167 - val_loss: 0.5658 - val_accuracy: 0.9375\n",
            "Epoch 30/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.4704 - accuracy: 0.8167 - val_loss: 0.8424 - val_accuracy: 0.5625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "class_labels = ['French bulldog', 'German shepherd', 'Golden retriever', 'Poodle']\n",
        "\n",
        "def predict_image(img_path, model, threshold=0.6):\n",
        "    img = image.load_img(img_path, target_size=(150, 150))\n",
        "    img_tensor = image.img_to_array(img)\n",
        "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "    img_tensor /= 255.\n",
        "\n",
        "    prediction = model.predict(img_tensor)\n",
        "    pred_class = np.argmax(prediction, axis=1)[0]\n",
        "    pred_label = class_labels[pred_class]\n",
        "\n",
        "    if np.max(prediction) < threshold:\n",
        "        pred_label = 'other'\n",
        "    return pred_label\n",
        "\n",
        "img_path = '/content/drive/MyDrive/7 10 2017/Mixed-Breeds-Of-Poodles-Best-scaled-e1586976423144.jpg'\n",
        "prediction = predict_image(img_path, model)\n",
        "print(f'Prediction: {prediction}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdYFkPslOKk3",
        "outputId": "9e174e7b-8dd3-4ff7-a5e3-a0fca9c0f3a3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "Prediction: Poodle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UqDLZ6A8VZ5R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}